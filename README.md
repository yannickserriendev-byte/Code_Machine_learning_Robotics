# ğŸ§  Real-Time Multi-Task Tactile Sensing Pipeline

**End-to-end workflow for tactile sensing research using photoelastic fringe patterns and deep learning.**

## ğŸš€ Quick Start (3 Steps)

1. **Acquire Data**: Run acquisition pipeline to collect sensor and image data
2. **Prepare & Augment**: Preprocess and augment datasets for ML training
3. **Train & Infer**: Train models, run inference, and analyze results


## ğŸ§© Simple Pipeline: File Execution Sequence
Run these files in order for a complete workflow:

1. `acquisition/main_acquisition_pipeline.py`  
	*Orchestrates the entire tactile sensing experiment: interactive setup, synchronized force sensor and camera data collection, robust file organization, dropped frame diagnostics, force-to-image mapping, optional filtering, spatial labeling, and visualization. Produces a complete, ML-ready dataset from hardware.*
2. `preprocessing/combine_full_dataset.py`  
	*Combines multiple trial folders into a unified dataset. Merges images and sensor measurements, matches force and position data, renames and renumbers files, and creates a single metadata CSV for downstream ML. Handles missing data and enforces consistent structure.*
3. `augmentation/augment_images_desktop_and_super_computer.py`  
	*Generates diverse augmented image-label pairs for ML. Applies geometric and color transformations, noise injection, and masking. Tracks contact points through transformations, supports both desktop and supercomputer batch processing, and outputs robust, timestamped CSVs.*
4. `postprocessing/postprocess_labels.py`  
	*Cleans and standardizes augmented label CSVs: converts forces to absolute values, sets shape/position to None/NaN for no-contact frames, renames columns, sorts by image/augmentation, remaps shapes to integer classes, and saves a final, ML-compatible CSV.*
5. `training/train_multitask_model.py`  
	*Trains and evaluates multi-task deep learning models (ResNet18 or custom CNN) for force, class, and contact point prediction. Loads config and dataset, splits data, runs training loop with validation, checkpoints, computes metrics, and saves results/plots.*
6. `inference/main.py`  
	*Runs real-time or batch inference using trained models on new or live camera data. Dynamically loads model and dataset, preprocesses images, predicts force/class/contact point, and outputs results. Supports both grayscale/RGB and CPU/GPU modes.*

---

## ğŸ“‹ Requirements

**Hardware:**
- NI-DAQ device with 6-axis force sensor (for acquisition)
- Basler camera for tactile imaging (for acquisition)

**Software:**
```bash
pip install torch torchvision pandas matplotlib scikit-learn seaborn pillow nidaqmx pypylon opencv-python scipy numpy
```

## ğŸ”„ Complete Workflow

1. **ğŸ“‹ Data Acquisition** - Collect synchronized force and image data
2. **ğŸ§¹ Preprocessing** - Combine and clean raw datasets
3. **ğŸ§¬ Augmentation** - Expand dataset with robust image/label transformations
4. **ğŸ“Š Postprocessing** - Analyze, visualize, and refine model outputs
5. **ğŸ§  Model Training** - Multi-task learning for force, class, and contact point
6. **ğŸ”® Inference** - Predict on new data or live camera input


## ğŸ“ Output Structure

```
Project/
â”œâ”€â”€ acquisition/
â”‚   â””â”€â”€ Trial_XXX/
â”‚       â”œâ”€â”€ measurement_setup_data.csv
â”‚       â”œâ”€â”€ Images/
â”‚       â”œâ”€â”€ force.csv
â”‚       â”œâ”€â”€ *_timestamps.csv
â”‚       â””â”€â”€ Results/
â”‚           â”œâ”€â”€ unfiltered_final_frame_force_mapping.csv
â”‚           â”œâ”€â”€ filtered_final_frame_force_mapping.csv
â”‚           â”œâ”€â”€ positions_indentation.csv
â”‚           â””â”€â”€ highlighted_forces_dual_axis.png
â”œâ”€â”€ preprocessing/
â”‚   â””â”€â”€ full_dataset.csv
â”œâ”€â”€ augmentation/
â”‚   â””â”€â”€ augmented_images/
â”‚   â””â”€â”€ augmented_labels.csv
â”œâ”€â”€ training/
â”‚   â””â”€â”€ model_weights.pth
â”‚   â””â”€â”€ training_loss_plot.png
â”‚   â””â”€â”€ metrics_summary.json
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ predictions.csv
â”‚   â””â”€â”€ live_results/
â”œâ”€â”€ postprocessing/
â”‚   â””â”€â”€ analysis_plots/
â”‚   â””â”€â”€ processed_labels.csv
```

## âš™ï¸ Key Configuration

**Edit these parameters in the relevant scripts:**

```python
# Acquisition
acquisition_time = 15
force_acquisition_frequency = 5000
acquisition_frequency_camera = 50
Max_Number_of_allowed_lost_frames = 55

# Preprocessing/Augmentation
input_data_path = "acquisition/Trial_XXX/Results/"
output_data_path = "preprocessing/full_dataset.csv"

# Training
MODEL_TYPE = "resnet18"  # or "owncnn"
BATCH_SIZE = 32
NUM_EPOCHS = 50
LEARNING_RATE = 1e-3

# Inference
model_path = "training/model_weights.pth"
input_images = "inference/new_images/"
```

## ğŸ”§ Individual Modules

| Module/Folder      | Purpose                                 | When to Use Separately                |
|--------------------|-----------------------------------------|---------------------------------------|
| acquisition/       | Data collection from hardware           | New experiments, hardware changes     |
| preprocessing/     | Dataset combination and cleaning        | Custom dataset formats                |
| augmentation/      | Image/label augmentation                | Robustness testing, new transforms    |
| training/          | Model training and evaluation           | Model architecture changes            |
| inference/         | Run trained models on new data          | Live demo, batch prediction           |
| postprocessing/    | Output analysis and visualization       | Custom analysis, publication figures  |

## ğŸ”§ Troubleshooting

**Common Issues:**
- **Hardware not detected**: Check connections and drivers
- **Frame loss**: Lower camera frequency or check system performance
- **Model training fails**: Check dataset paths and config
- **Inference errors**: Verify model weights and input formats

**Quality Indicators:**
- Frame loss < threshold (see acquisition README)
- Consistent force/image synchronization
- Model metrics meet research targets

---

ğŸ’¡ **Pro Tip**: Start with small datasets and short acquisition times to validate your setup before scaling up.

For detailed parameter explanations and advanced configuration, see inline documentation in each module's main script.
